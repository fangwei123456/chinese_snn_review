\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand*\HyPL@Entry[1]{}
\citation{deep-learning-nature}
\citation{he2015delving,szegedy2015going,he2016deep,dosovitskiy2020image}
\citation{girshick2014rich,redmon2016you}
\citation{graves2013speech,graves2013hybrid}
\citation{sutskever2014sequence,bahdanau2014neural,sennrich2015neural}
\citation{mnih2015human,silver2017mastering}
\citation{NEURIPS2020_1457c0d6,zeng2021pangualphalargescaleautoregressivepretrained,openai2024gpt4technicalreport}
\citation{NIPS2014_5ca3e9b1,DBLP:journals/corr/RadfordMC15,Rombach_2022_CVPR}
\citation{hassabis2017neuroscience,Zador2023}
\citation{rosenblatt1958perceptron}
\citation{rumelhart1986learning}
\citation{cortes1995support}
\citation{maass1997networks}
\citation{gewaltig2007nest,spaun,Stimberg2019}
\HyPL@Entry{0<</S/D>>}
\@writefile{toc}{\contentsline {section}{\numberline {1}引言}{1}{section.1}\protected@file@percent }
\citation{mead1990neuromorphic,roy2019towards}
\citation{lichtsteiner2008128}
\citation{dong2017spike}
\citation{merolla2014million}
\citation{loihi}
\citation{ma2017darwin}
\citation{pei2019towards}
\citation{roy2019towards}
\citation{10.1162/neco.2006.18.7.1527}
\citation{MNIST}
\citation{Goodfellow-et-al-2016}
\citation{NIPS2012_c399862d}
\citation{russakovsky2015imagenet}
\citation{neftci2019surrogate}
\citation{cao2015spiking}
\citation{TAVANAEI201947}
\citation{mead1990neuromorphic}
\citation{maass1997networks}
\citation{lichtsteiner2008128}
\citation{merolla2014million}
\citation{ma2017darwin}
\citation{dong2017spike}
\citation{pei2019towards}
\citation{wu2018STBP}
\citation{10.1162/neco_a_01086}
\citation{shrestha2018slayer}
\citation{doi:10.1126/sciadv.adi1480}
\citation{SEWResNet}
\citation{zhou2023spikformer}
\citation{NEURIPS2023_ca0f5358}
\citation{Yao2024}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces \baselineskip 10pt\fontsize  {9pt}{\baselineskip }\selectfont  \bf  梯度替代学习算法发展历程\relax }}{3}{figure.caption.2}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces \baselineskip 10pt \relax }}{3}{figure.caption.2}\protected@file@percent }
\providecommand*\caption@xref[2]{\@setref\relax\@undefined{#1}}
\newlabel{fig: timeline}{{1}{3}{\baselineskip 10pt \relax }{figure.caption.2}{}}
\citation{ioffe2015batch}
\citation{ba2016layer}
\citation{izhikevich2003simple}
\citation{fang2021incorporating,doi:10.1126/sciadv.adi1480}
\citation{ledinauskas2020training}
\citation{Bodo2017Conversion}
\@writefile{toc}{\contentsline {section}{\numberline {2}SNN的基本组分和评测基准}{4}{section.2}\protected@file@percent }
\newlabel{eq, continuous LIF neuronal charge}{{1}{4}{SNN的基本组分和评测基准}{equation.2.1}{}}
\newlabel{eq, continuous neuronal fire}{{2}{4}{SNN的基本组分和评测基准}{equation.2.2}{}}
\newlabel{eq, continuous neuronal reset}{{3}{4}{SNN的基本组分和评测基准}{equation.2.3}{}}
\newlabel{eq, discrete neuronal charge}{{4}{4}{SNN的基本组分和评测基准}{equation.2.4}{}}
\newlabel{eq, discrete neuronal fire}{{5}{4}{SNN的基本组分和评测基准}{equation.2.5}{}}
\newlabel{eq, discrete neuronal reset}{{6}{4}{SNN的基本组分和评测基准}{equation.2.6}{}}
\newlabel{eq, discrete LIF neuronal charge}{{7}{4}{SNN的基本组分和评测基准}{equation.2.7}{}}
\citation{MNIST}
\citation{FMNIST}
\citation{CIFAR10}
\citation{russakovsky2015imagenet}
\citation{wu2018STBP,fang2021incorporating,doi:10.1126/sciadv.adi1480}
\citation{10.3389/fnins.2015.00437}
\citation{10.3389/fnins.2017.00309}
\citation{amir2017low}
\citation{Bi_2019_ICCV}
\citation{10.3389/fnins.2015.00437}
\citation{10.3389/fnins.2021.726582}
\citation{shd}
\citation{10.3389/fnins.2021.608567}
\citation{yin2021accurate,fang2023parallel}
\citation{pei2019towards,Yao2024,Yang2024}
\citation{wu2018STBP}
\citation{10.1162/neco_a_01086}
\citation{shrestha2018slayer}
\citation{wu2018STBP}
\citation{10.1162/neco_a_01086}
\citation{fang2021incorporating}
\citation{Zenke2020.06.29.176925}
\citation{li2021differentiable}
\@writefile{toc}{\contentsline {section}{\numberline {3}深度脉冲神经网络的梯度替代训练算法}{5}{section.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}基础学习算法}{5}{subsection.3.1}\protected@file@percent }
\citation{wu2021tandem}
\citation{qiu2024self}
\citation{wu2021tandem}
\citation{qiu2024self}
\citation{wu2021tandem}
\citation{kheradpisheh2022spiking}
\citation{wu2021tandem}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces \baselineskip 10pt\fontsize  {9pt}{\baselineskip }\selectfont  \bf  两类ANN辅助训练方法\relax }}{6}{figure.caption.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces \baselineskip 10pt Two types of ANN-assisted training methods. (a)Training based on shared weights\cite  {wu2021tandem}; (b)Training based on distillation\cite  {qiu2024self}.\relax }}{6}{figure.caption.3}\protected@file@percent }
\newlabel{fig:assist}{{2}{6}{\baselineskip 10pt Two types of ANN-assisted training methods. (a)Training based on shared weights\cite {wu2021tandem}; (b)Training based on distillation\cite {qiu2024self}.\relax }{figure.caption.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}ANN辅助训练算法}{6}{subsection.3.2}\protected@file@percent }
\citation{kheradpisheh2022spiking}
\citation{wu2021tandem}
\citation{kheradpisheh2022spiking}
\citation{xu2023constructing}
\citation{qiu2024self}
\citation{xu2023constructing}
\citation{qiu2024self}
\citation{fang2021incorporating}
\citation{yao2022glif}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}神经元和突触改进}{8}{subsection.3.3}\protected@file@percent }
\newlabel{eq, plif charge}{{22}{8}{神经元和突触改进}{equation.3.22}{}}
\citation{ijcai2022p343}
\citation{huang2024clif}
\citation{fang2023parallel}
\citation{fang2023parallel}
\citation{fang2023parallel}
\citation{fang2023parallel}
\newlabel{eq, clif c update}{{26}{9}{神经元和突触改进}{equation.3.26}{}}
\newlabel{eq, clif reset}{{27}{9}{神经元和突触改进}{equation.3.27}{}}
\newlabel{eq, psn neuronal charge}{{28}{9}{神经元和突触改进}{equation.3.28}{}}
\citation{10191884}
\citation{8351295}
\citation{BOHTE200217}
\citation{8050527}
\citation{mostafa2017supervised}
\citation{kheradpisheh2020temporal}
\citation{8050527}
\citation{fang2021incorporating}
\citation{yao2022glif}
\citation{ijcai2022p343}
\citation{huang2024clif}
\citation{fang2023parallel}
\citation{SNN-IIR}
\citation{hammouamri2024learning}
\citation{he2016deep}
\citation{he2016deep}
\citation{hu2020spiking}
\citation{zheng2020going}
\citation{SEWResNet}
\citation{SEWResNet}
\citation{10428029}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}网络结构改进}{10}{subsection.3.4}\protected@file@percent }
\citation{graves2014generatingsequencesrecurrentneural,bahdanau2014neural,vaswani2017attention}
\citation{Yao_2021_ICCV}
\citation{Yao_2023_ICCV,yao2023sparser,xu2023enhancing}
\citation{10032591,zhu2022tcja}
\citation{yao2023sparser,10032591,Yao_2023_ICCV,Yao2024}
\citation{huang2022tada}
\citation{wang2018non}
\citation{Yao_2023_ICCV}
\citation{Yao2024,kim2023c,chang202373}
\citation{Yao2024}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces \baselineskip 10pt\fontsize  {9pt}{\baselineskip }\selectfont  \bf  脉冲神经元分类任务仿真步数和正确率(\%)\relax }}{11}{table.caption.4}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces \baselineskip 10pt The time-steps and classification accuracy (\%) of spiking neuron models\relax }}{11}{table.caption.4}\protected@file@percent }
\newlabel{tab: neuron acc}{{1}{11}{\baselineskip 10pt The time-steps and classification accuracy (\%) of spiking neuron models\relax }{table.caption.4}{}}
\citation{vaswani2017attention}
\citation{zhang2022spiking,zhang2022spike,han2023complex}
\citation{zhou2023spikformer}
\citation{zhou2023spikformer}
\citation{NEURIPS2023_ca0f5358}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces \baselineskip 10pt\fontsize  {9pt}{\baselineskip }\selectfont  \bf  常见的残差块结构\relax }}{12}{figure.caption.5}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces \baselineskip 10pt The commonly used residual blocks\relax }}{12}{figure.caption.5}\protected@file@percent }
\newlabel{fig: res block}{{3}{12}{\baselineskip 10pt The commonly used residual blocks\relax }{figure.caption.5}{}}
\citation{Shi_2024_CVPR}
\citation{Shi_2024_CVPR}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces \baselineskip 10pt\fontsize  {9pt}{\baselineskip }\selectfont  \bf  深度SNN中的自注意力机制\relax }}{13}{figure.caption.6}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces \baselineskip 10pt Self attention mechanisms in deep SNNs\relax }}{13}{figure.caption.6}\protected@file@percent }
\newlabel{fig: self attention}{{4}{13}{\baselineskip 10pt Self attention mechanisms in deep SNNs\relax }{figure.caption.6}{}}
\citation{zhou2024qkformerhierarchicalspikingtransformer}
\citation{zhou2023spikformer}
\citation{NEURIPS2023_ca0f5358}
\citation{hassani2022escapingbigdataparadigm}
\citation{Shi_2024_CVPR}
\citation{yao2024spikedriven}
\citation{NEURIPS2023_ca0f5358}
\citation{zhou2024qkformerhierarchicalspikingtransformer}
\citation{liu2021swin}
\citation{6757323}
\citation{na2022autosnn}
\citation{kim2022neural}
\citation{che2022differentiable}
\citation{ioffe2015batch}
\citation{ba2016layer}
\citation{neunorm}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces \baselineskip 10pt\fontsize  {9pt}{\baselineskip }\selectfont  \bf  常见深度SNN架构在ImageNet数据集的分类正确率、功耗和参数量\relax }}{15}{figure.caption.7}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces \baselineskip 10pt The classification accuracy, power consumption, and parameters of mainstream deep SNN structures\relax }}{15}{figure.caption.7}\protected@file@percent }
\newlabel{fig: network structure pow param acc}{{5}{15}{\baselineskip 10pt The classification accuracy, power consumption, and parameters of mainstream deep SNN structures\relax }{figure.caption.7}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5}正则化方法}{15}{subsection.3.5}\protected@file@percent }
\citation{zheng2020going}
\citation{10.3389/fnins.2021.773954}
\citation{duan2022temporal}
\citation{Guo_2023_ICCV_mpbn}
\citation{ioffe2015batch}
\citation{zheng2020going}
\citation{10.3389/fnins.2021.773954}
\citation{duan2022temporal}
\citation{Guo_2023_ICCV_rmp}
\citation{deng2022temporal}
\citation{li2022neuromorphic}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces \baselineskip 10pt\fontsize  {9pt}{\baselineskip }\selectfont  \bf  深度SNN中的批量标准化类方法\relax }}{16}{table.caption.8}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces \baselineskip 10pt Batch normalization methods in deep SNNs\relax }}{16}{table.caption.8}\protected@file@percent }
\newlabel{tab: bn methods}{{2}{16}{\baselineskip 10pt Batch normalization methods in deep SNNs\relax }{table.caption.8}{}}
\citation{zhang2020temporal}
\citation{zhu2022training}
\citation{zhu2024exploring}
\citation{kaiser2020synaptic}
\citation{xiao2022online}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.6}事件驱动学习算法}{17}{subsection.3.6}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {3.7}在线学习算法}{17}{subsection.3.7}\protected@file@percent }
\newlabel{eq_online_trace}{{45}{17}{在线学习算法}{equation.3.45}{}}
\citation{meng2022training}
\citation{meng2023towards}
\citation{jiang2024ndot}
\citation{zhu2024online}
\citation{hu2024highperformance}
\citation{liu2022convnet}
\citation{perez-nieves2021sparse}
\citation{doi:10.1126/sciadv.adi1480}
\citation{NEURIPS2023_b9f253c2}
\citation{fang2023parallel}
\newlabel{eq_OSR_1}{{47}{18}{在线学习算法}{equation.3.47}{}}
\newlabel{eq_OSR_2}{{48}{18}{在线学习算法}{equation.3.48}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.8}训练加速方法}{18}{subsection.3.8}\protected@file@percent }
\citation{fang2023parallel}
\citation{wu2021tandem}
\citation{xu2023constructing}
\citation{huang2024clif}
\citation{fang2023parallel}
\citation{duan2022temporal}
\citation{zhu2024online}
\citation{NEURIPS2023_b9f253c2}
\@writefile{toc}{\contentsline {section}{\numberline {4}综合对比实验}{19}{section.4}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}分类任务性能}{19}{subsection.4.1}\protected@file@percent }
\citation{wu2021tandem}
\citation{xu2023constructing}
\citation{xu2023constructing}
\citation{huang2024clif}
\citation{fang2023parallel}
\citation{duan2022temporal}
\citation{zhu2024online}
\citation{NEURIPS2023_b9f253c2}
\citation{doi:10.1126/sciadv.adi1480}
\citation{fang2023parallel}
\citation{NEURIPS2023_b9f253c2}
\citation{doi:10.1126/sciadv.adi1480}
\citation{fang2023parallel}
\citation{NEURIPS2023_b9f253c2}
\citation{fang2023parallel}
\citation{NEURIPS2023_b9f253c2}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces \baselineskip 10pt\fontsize  {9pt}{\baselineskip }\selectfont  \bf  对比各类代表性方法任务正确率(\%)\relax }}{20}{table.caption.9}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces \baselineskip 10pt Task accuracy (\%) of representative methods\relax }}{20}{table.caption.9}\protected@file@percent }
\newlabel{tab: cmp acc}{{3}{20}{\baselineskip 10pt Task accuracy (\%) of representative methods\relax }{table.caption.9}{}}
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces \baselineskip 10pt\fontsize  {9pt}{\baselineskip }\selectfont  \bf  对比加速方法性能\relax }}{20}{table.caption.10}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {4}{\ignorespaces \baselineskip 10pt Speedup ratios of acceleration methods\relax }}{20}{table.caption.10}\protected@file@percent }
\newlabel{tab: cmp speedup}{{4}{20}{\baselineskip 10pt Speedup ratios of acceleration methods\relax }{table.caption.10}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}加速性能测试}{20}{subsection.4.2}\protected@file@percent }
\citation{rathi2021diet}
\citation{SEWResNet,zhou2023spikformer,NEURIPS2023_ca0f5358}
\citation{fang2021incorporating}
\citation{Yao_2023_ICCV}
\citation{hu2024highperformance}
\citation{adrian1926impulses}
\citation{johansson2004first}
\citation{SEWResNet}
\citation{zhou2023spikformer}
\citation{NEURIPS2023_ca0f5358}
\citation{fang2021incorporating,ijcai2022p343,fang2023parallel,huang2024clif}
\citation{maass1997networks}
\citation{he2024network}
\@writefile{toc}{\contentsline {section}{\numberline {5}研究挑战与未来研究方向}{21}{section.5}\protected@file@percent }
\citation{zhou2023spikformer}
\citation{NEURIPS2023_ca0f5358}
\citation{gilbert2013top}
\citation{yin2021accurate}
\citation{rao2022long}
\citation{hebb1949the}
\citation{bi1998synaptic}
\citation{imam2020rapid}
\citation{loihi}
\citation{wu2022brain}
\citation{pei2019towards}
\citation{huang2024clif}
\citation{fang2023parallel}
\citation{SNN-IIR}
\citation{doi:10.1126/sciadv.adi1480}
\citation{perez-nieves2021sparse}
\@writefile{toc}{\contentsline {section}{\numberline {6}总结与展望}{22}{section.6}\protected@file@percent }
\bibstyle{unsrt}
\bibdata{ref}
\bibcite{deep-learning-nature}{1}
\bibcite{he2015delving}{2}
\bibcite{szegedy2015going}{3}
\bibcite{he2016deep}{4}
\bibcite{dosovitskiy2020image}{5}
\bibcite{girshick2014rich}{6}
\bibcite{redmon2016you}{7}
\bibcite{graves2013speech}{8}
\bibcite{graves2013hybrid}{9}
\bibcite{sutskever2014sequence}{10}
\bibcite{bahdanau2014neural}{11}
\bibcite{sennrich2015neural}{12}
\bibcite{mnih2015human}{13}
\bibcite{silver2017mastering}{14}
\bibcite{NEURIPS2020_1457c0d6}{15}
\bibcite{zeng2021pangualphalargescaleautoregressivepretrained}{16}
\bibcite{openai2024gpt4technicalreport}{17}
\bibcite{NIPS2014_5ca3e9b1}{18}
\bibcite{DBLP:journals/corr/RadfordMC15}{19}
\bibcite{Rombach_2022_CVPR}{20}
\bibcite{hassabis2017neuroscience}{21}
\bibcite{Zador2023}{22}
\bibcite{rosenblatt1958perceptron}{23}
\bibcite{rumelhart1986learning}{24}
\bibcite{cortes1995support}{25}
\bibcite{maass1997networks}{26}
\bibcite{gewaltig2007nest}{27}
\bibcite{spaun}{28}
\bibcite{Stimberg2019}{29}
\bibcite{mead1990neuromorphic}{30}
\bibcite{roy2019towards}{31}
\bibcite{lichtsteiner2008128}{32}
\bibcite{dong2017spike}{33}
\bibcite{merolla2014million}{34}
\bibcite{loihi}{35}
\bibcite{ma2017darwin}{36}
\bibcite{pei2019towards}{37}
\bibcite{10.1162/neco.2006.18.7.1527}{38}
\bibcite{MNIST}{39}
\bibcite{Goodfellow-et-al-2016}{40}
\bibcite{NIPS2012_c399862d}{41}
\bibcite{russakovsky2015imagenet}{42}
\bibcite{neftci2019surrogate}{43}
\bibcite{cao2015spiking}{44}
\bibcite{TAVANAEI201947}{45}
\bibcite{wu2018STBP}{46}
\bibcite{10.1162/neco_a_01086}{47}
\bibcite{shrestha2018slayer}{48}
\bibcite{doi:10.1126/sciadv.adi1480}{49}
\bibcite{SEWResNet}{50}
\bibcite{zhou2023spikformer}{51}
\bibcite{NEURIPS2023_ca0f5358}{52}
\bibcite{Yao2024}{53}
\bibcite{ioffe2015batch}{54}
\bibcite{ba2016layer}{55}
\bibcite{izhikevich2003simple}{56}
\bibcite{fang2021incorporating}{57}
\bibcite{ledinauskas2020training}{58}
\bibcite{Bodo2017Conversion}{59}
\bibcite{FMNIST}{60}
\bibcite{CIFAR10}{61}
\bibcite{10.3389/fnins.2015.00437}{62}
\bibcite{10.3389/fnins.2017.00309}{63}
\bibcite{amir2017low}{64}
\bibcite{Bi_2019_ICCV}{65}
\bibcite{10.3389/fnins.2021.726582}{66}
\bibcite{shd}{67}
\bibcite{10.3389/fnins.2021.608567}{68}
\bibcite{yin2021accurate}{69}
\bibcite{fang2023parallel}{70}
\bibcite{Yang2024}{71}
\bibcite{Zenke2020.06.29.176925}{72}
\bibcite{li2021differentiable}{73}
\bibcite{wu2021tandem}{74}
\bibcite{qiu2024self}{75}
\bibcite{kheradpisheh2022spiking}{76}
\bibcite{xu2023constructing}{77}
\bibcite{yao2022glif}{78}
\bibcite{ijcai2022p343}{79}
\bibcite{huang2024clif}{80}
\bibcite{10191884}{81}
\bibcite{8351295}{82}
\bibcite{BOHTE200217}{83}
\bibcite{8050527}{84}
\bibcite{mostafa2017supervised}{85}
\bibcite{kheradpisheh2020temporal}{86}
\bibcite{SNN-IIR}{87}
\bibcite{hammouamri2024learning}{88}
\bibcite{hu2020spiking}{89}
\bibcite{zheng2020going}{90}
\bibcite{10428029}{91}
\bibcite{graves2014generatingsequencesrecurrentneural}{92}
\bibcite{vaswani2017attention}{93}
\bibcite{Yao_2021_ICCV}{94}
\bibcite{Yao_2023_ICCV}{95}
\bibcite{yao2023sparser}{96}
\bibcite{xu2023enhancing}{97}
\bibcite{10032591}{98}
\bibcite{zhu2022tcja}{99}
\bibcite{huang2022tada}{100}
\bibcite{wang2018non}{101}
\bibcite{kim2023c}{102}
\bibcite{chang202373}{103}
\bibcite{zhang2022spiking}{104}
\bibcite{zhang2022spike}{105}
\bibcite{han2023complex}{106}
\bibcite{Shi_2024_CVPR}{107}
\bibcite{zhou2024qkformerhierarchicalspikingtransformer}{108}
\bibcite{hassani2022escapingbigdataparadigm}{109}
\bibcite{yao2024spikedriven}{110}
\bibcite{liu2021swin}{111}
\bibcite{6757323}{112}
\bibcite{na2022autosnn}{113}
\bibcite{kim2022neural}{114}
\bibcite{che2022differentiable}{115}
\bibcite{neunorm}{116}
\bibcite{10.3389/fnins.2021.773954}{117}
\bibcite{duan2022temporal}{118}
\bibcite{Guo_2023_ICCV_mpbn}{119}
\bibcite{Guo_2023_ICCV_rmp}{120}
\bibcite{deng2022temporal}{121}
\bibcite{li2022neuromorphic}{122}
\bibcite{zhang2020temporal}{123}
\bibcite{zhu2022training}{124}
\bibcite{zhu2024exploring}{125}
\bibcite{kaiser2020synaptic}{126}
\bibcite{xiao2022online}{127}
\bibcite{meng2022training}{128}
\bibcite{meng2023towards}{129}
\bibcite{jiang2024ndot}{130}
\bibcite{zhu2024online}{131}
\bibcite{hu2024highperformance}{132}
\bibcite{liu2022convnet}{133}
\bibcite{perez-nieves2021sparse}{134}
\bibcite{NEURIPS2023_b9f253c2}{135}
\bibcite{rathi2021diet}{136}
\bibcite{adrian1926impulses}{137}
\bibcite{johansson2004first}{138}
\bibcite{he2024network}{139}
\bibcite{gilbert2013top}{140}
\bibcite{rao2022long}{141}
\bibcite{hebb1949the}{142}
\bibcite{bi1998synaptic}{143}
\bibcite{imam2020rapid}{144}
\bibcite{wu2022brain}{145}
\newlabel{LastPage}{{}{32}{}{page.32}{}}
\xdef\lastpage@lastpage{32}
\xdef\lastpage@lastpageHy{32}
\gdef \@abspage@last{32}
